{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TGI: (Production-grade, 안정성 중심)"
      ],
      "metadata": {
        "id": "gIMlMC8toq4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Serving"
      ],
      "metadata": {
        "id": "0b5YlKz0otEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLdlz_vcohzg"
      },
      "outputs": [],
      "source": [
        "docker run --gpus all \\\n",
        "  --shm-size 1g \\\n",
        "  -p 8080:80 \\\n",
        "  -v ~/.cache/huggingface:/data \\\n",
        "  ghcr.io/huggingface/text-generation-inference:latest \\\n",
        "  --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
        "  --max-total-tokens 4096 \\\n",
        "  --max-input-length 3072 \\\n",
        "  --max-batch-total-tokens 8192 \\\n",
        "  --waiting-served-ratio 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client Inference"
      ],
      "metadata": {
        "id": "dkDCzcv2oxWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(model=\"http://localhost:8080\")\n",
        "\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story about space exploration.\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "q9_jSOLeowFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Compatible API"
      ],
      "metadata": {
        "id": "qQl-XJzio4Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8080/v1\",\n",
        "    api_key=\"not-needed\",\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story.\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "KIvu-6Klo1TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM (최대 처리량, PagedAttention)"
      ],
      "metadata": {
        "id": "fF3O0am2pEO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serving Model"
      ],
      "metadata": {
        "id": "dujr9pOEpHj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm\n",
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  --gpu-memory-utilization 0.85 \\\n",
        "  --max-num-batched-tokens 8192"
      ],
      "metadata": {
        "id": "v6TamitGo9dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client Inference OpenAI API"
      ],
      "metadata": {
        "id": "R_MdhvlppRfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"not-needed\",\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story.\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "omd7wEPnpKqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama.cpp"
      ],
      "metadata": {
        "id": "lBOQTzNdqby8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build & Model Checkpoint Download"
      ],
      "metadata": {
        "id": "-QofcJnJqg14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp\n",
        "make"
      ],
      "metadata": {
        "id": "FBv7RSKEqEyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curl -L -O \\\n",
        "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "LuLehUYMqnTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serve (Quantization)"
      ],
      "metadata": {
        "id": "i_yPHpVMqqcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "./server \\\n",
        "  -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8080 \\\n",
        "  -c 4096 \\\n",
        "  --threads 8 \\\n",
        "  --batch-size 512 \\\n",
        "  --n-gpu-layers 0"
      ],
      "metadata": {
        "id": "dbJcogQHqtT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client Inference"
      ],
      "metadata": {
        "id": "7-PKssqSqxvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8080/v1\",\n",
        "    api_key=\"sk-no-key-required\",\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"smollm2\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story.\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "POT9faHWquzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}